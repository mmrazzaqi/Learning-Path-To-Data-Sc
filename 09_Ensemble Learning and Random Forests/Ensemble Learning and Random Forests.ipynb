{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b75047",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "If we aggregate\n",
    "the predictions of a group of predictors (such as classifiers or regressors), you will\n",
    "often get better predictions than with the best individual predictor. A group of predictors\n",
    "is called an **ensemble**; thus, this technique is called **Ensemble Learning**, and an\n",
    "Ensemble Learning algorithm is called an **Ensemble method**.\n",
    "\n",
    "Such an ensemble (Decision Tree classifiers) of Decision Trees is called a **Random Forest**\n",
    "\n",
    "Popular Ensemble methods, including **bagging,\n",
    "boosting, stacking**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff3100",
   "metadata": {},
   "source": [
    "## Voting Classifiers\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/hard_voting_clf.jpg\" width='600' />\n",
    "\n",
    "Voting classifier often achieves a higher accuracy than the\n",
    "best classifier in the ensemble.\n",
    "\n",
    "Suppose you build an ensemble containing 1,000 classifiers that are individually\n",
    "correct only 51% of the time (barely better than random guessing). If you predict\n",
    "the majority voted class, you can hope for up to 75% accuracy! However, this is\n",
    "only true if all classifiers are perfectly independent, making uncorrelated errors,\n",
    "which is clearly not the case since they are trained on the same data. They are likely to\n",
    "make the same types of errors, so there will be many majority votes for the wrong\n",
    "class, reducing the ensemble’s accuracy.\n",
    "- Ensemble methods work best when the predictors are as independent\n",
    "from one another as possible. One way to get diverse classifiers\n",
    "is to train them using very different algorithms. This increases the\n",
    "chance that they will make very different types of errors, improving\n",
    "the ensemble’s accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af112617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eef8e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f50253de",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d79be76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svm', SVC())])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(estimators= [('lr', log_clf), ('rf', rnd_clf), ('svm', svm_clf)], \n",
    "                              voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4f02e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.888\n",
      "SVC 0.896\n",
      "VotingClassifier 0.904\n"
     ]
    }
   ],
   "source": [
    "# Let’s look at each classifier’s accuracy on the test set\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca41a731",
   "metadata": {},
   "source": [
    "If all classifiers are able to estimate class probabilities (i.e., they have a pre\n",
    "dict_proba() method), then we can predict the class with the\n",
    "highest class probability, averaged over all the individual classifiers. This is called **soft\n",
    "voting.** <br>\n",
    "It often achieves higher performance than hard voting because it gives more\n",
    "weight to highly confident votes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c795784",
   "metadata": {},
   "source": [
    "## Bagging and Pasting\n",
    "\n",
    "One way to get a diverse set of classifiers is to use very different training algorithms. <br>\n",
    "**Bagging:** Another approach is to use the same training algorithm for every predictor, but to train them on **different random subsets of the training set.**\n",
    "- When sampling is performed with replacement, this method is called bagging (short for\n",
    "bootstrap aggregating). <br>\n",
    "- When sampling is performed without replacement, it is called **pasting.**\n",
    "\n",
    "**Sample with replacement:** The two sample values are independent. Practically, this means that what we get on the first one doesn't affect what we get on the second. Mathematically, this means that the covariance between the two is zero. <br>\n",
    "The outcome of the first draw does not affect the probability of the outcome on the second draw.\n",
    "\n",
    "<br>\n",
    "\n",
    "When we **sample without replacement**, the items in the sample are dependent because the outcome of one random draw is affected by the previous draw.\n",
    "\n",
    "<img src=\"images/bagging_1.png\" width='700' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d282ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), \n",
    "                            n_estimators=500, \n",
    "                            max_samples=100,\n",
    "                            bootstrap=True,\n",
    "                            n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26d0a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fae1f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a61f9c3",
   "metadata": {},
   "source": [
    "The BaggingClassifier automatically performs soft voting\n",
    "instead of hard voting if the base classifier can estimate class probabilities\n",
    "(i.e., if it has a predict_proba() method), which is the case\n",
    "with Decision Trees classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81b3296",
   "metadata": {},
   "source": [
    "## Out-of-Bag Evaluation\n",
    "\n",
    "With bagging, some instances may be sampled several times for any given predictor,\n",
    "while others may not be sampled at all. <br>\n",
    "The remaining training instances that are not sampled are called out-of-bag (oob) instances. <br>\n",
    "Since a predictor never sees the oob instances during training, it can be evaluated on\n",
    "these instances, without the need for a separate validation set. We can evaluate the\n",
    "ensemble itself by averaging out the oob evaluations of each predictor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d3221cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=500,\n",
       "                  n_jobs=-1, oob_score=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# automatic oob evaluation after training\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                            n_estimators=500,\n",
    "                            bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4d67bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9013333333333333"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9f592ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let’s verify this accuracy of oob eveluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bd3f954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39411765, 0.60588235],\n",
       "       [0.39325843, 0.60674157],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00546448, 0.99453552],\n",
       "       [0.08121827, 0.91878173],\n",
       "       [0.32967033, 0.67032967],\n",
       "       [0.015625  , 0.984375  ],\n",
       "       [0.98895028, 0.01104972],\n",
       "       [0.96825397, 0.03174603],\n",
       "       [0.81578947, 0.18421053],\n",
       "       [0.        , 1.        ],\n",
       "       [0.79234973, 0.20765027],\n",
       "       [0.83236994, 0.16763006],\n",
       "       [0.95212766, 0.04787234],\n",
       "       [0.05847953, 0.94152047],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97752809, 0.02247191],\n",
       "       [0.94767442, 0.05232558],\n",
       "       [0.98930481, 0.01069519],\n",
       "       [0.01932367, 0.98067633],\n",
       "       [0.28961749, 0.71038251],\n",
       "       [0.87628866, 0.12371134],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94886364, 0.05113636],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.61340206, 0.38659794],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.16580311, 0.83419689],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.40860215, 0.59139785],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.20467836, 0.79532164],\n",
       "       [0.3015873 , 0.6984127 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0104712 , 0.9895288 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01176471, 0.98823529],\n",
       "       [0.97237569, 0.02762431],\n",
       "       [0.91975309, 0.08024691],\n",
       "       [0.96174863, 0.03825137],\n",
       "       [0.97765363, 0.02234637],\n",
       "       [0.0060241 , 0.9939759 ],\n",
       "       [0.07317073, 0.92682927],\n",
       "       [0.98843931, 0.01156069],\n",
       "       [0.00564972, 0.99435028],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01554404, 0.98445596],\n",
       "       [0.98907104, 0.01092896],\n",
       "       [0.82911392, 0.17088608],\n",
       "       [0.385     , 0.615     ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.68786127, 0.31213873],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.84102564, 0.15897436],\n",
       "       [1.        , 0.        ],\n",
       "       [0.65641026, 0.34358974],\n",
       "       [0.18579235, 0.81420765],\n",
       "       [0.63414634, 0.36585366],\n",
       "       [0.90555556, 0.09444444],\n",
       "       [0.        , 1.        ],\n",
       "       [0.20571429, 0.79428571],\n",
       "       [0.88135593, 0.11864407],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99481865, 0.00518135],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05102041, 0.94897959],\n",
       "       [0.05263158, 0.94736842],\n",
       "       [0.31578947, 0.68421053],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.83333333, 0.16666667],\n",
       "       [0.00518135, 0.99481865],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.25862069, 0.74137931],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.94915254, 0.05084746],\n",
       "       [0.78865979, 0.21134021],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.21827411, 0.78172589],\n",
       "       [0.64550265, 0.35449735],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01242236, 0.98757764],\n",
       "       [0.55367232, 0.44632768],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01136364, 0.98863636],\n",
       "       [1.        , 0.        ],\n",
       "       [0.32524272, 0.67475728],\n",
       "       [0.4375    , 0.5625    ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02645503, 0.97354497],\n",
       "       [0.9742268 , 0.0257732 ],\n",
       "       [0.31155779, 0.68844221],\n",
       "       [0.90291262, 0.09708738],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.79761905, 0.20238095],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01522843, 0.98477157],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99428571, 0.00571429],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.95384615, 0.04615385],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01477833, 0.98522167],\n",
       "       [0.28248588, 0.71751412],\n",
       "       [0.94382022, 0.05617978],\n",
       "       [0.23152709, 0.76847291],\n",
       "       [0.98857143, 0.01142857],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01694915, 0.98305085],\n",
       "       [0.71134021, 0.28865979],\n",
       "       [0.41968912, 0.58031088],\n",
       "       [0.41566265, 0.58433735],\n",
       "       [0.85393258, 0.14606742],\n",
       "       [0.93264249, 0.06735751],\n",
       "       [0.04812834, 0.95187166],\n",
       "       [0.81675393, 0.18324607],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01754386, 0.98245614],\n",
       "       [0.98830409, 0.01169591],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00662252, 0.99337748],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01666667, 0.98333333],\n",
       "       [0.00555556, 0.99444444],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.93452381, 0.06547619],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98913043, 0.01086957],\n",
       "       [0.        , 1.        ],\n",
       "       [0.49704142, 0.50295858],\n",
       "       [0.2311828 , 0.7688172 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.31693989, 0.68306011],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9798995 , 0.0201005 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.70454545, 0.29545455],\n",
       "       [0.89583333, 0.10416667],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99473684, 0.00526316],\n",
       "       [0.99428571, 0.00571429],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00588235, 0.99411765],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.10169492, 0.89830508],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03076923, 0.96923077],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02777778, 0.97222222],\n",
       "       [0.98888889, 0.01111111],\n",
       "       [0.95108696, 0.04891304],\n",
       "       [0.75      , 0.25      ],\n",
       "       [0.62162162, 0.37837838],\n",
       "       [0.        , 1.        ],\n",
       "       [0.18644068, 0.81355932],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95979899, 0.04020101],\n",
       "       [0.96511628, 0.03488372],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00990099, 0.99009901],\n",
       "       [0.        , 1.        ],\n",
       "       [0.33701657, 0.66298343],\n",
       "       [0.80346821, 0.19653179],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00505051, 0.99494949],\n",
       "       [0.00578035, 0.99421965],\n",
       "       [0.96410256, 0.03589744],\n",
       "       [0.        , 1.        ],\n",
       "       [0.2979798 , 0.7020202 ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98421053, 0.01578947],\n",
       "       [0.84269663, 0.15730337],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.08743169, 0.91256831],\n",
       "       [0.99450549, 0.00549451],\n",
       "       [0.03      , 0.97      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02259887, 0.97740113],\n",
       "       [0.99484536, 0.00515464],\n",
       "       [0.77040816, 0.22959184],\n",
       "       [0.00561798, 0.99438202],\n",
       "       [0.87431694, 0.12568306],\n",
       "       [0.98930481, 0.01069519],\n",
       "       [0.20212766, 0.79787234],\n",
       "       [0.22105263, 0.77894737],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.21546961, 0.78453039],\n",
       "       [0.94444444, 0.05555556],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99014778, 0.00985222],\n",
       "       [0.        , 1.        ],\n",
       "       [0.5212766 , 0.4787234 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00478469, 0.99521531],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.09625668, 0.90374332],\n",
       "       [0.11170213, 0.88829787],\n",
       "       [0.97619048, 0.02380952],\n",
       "       [0.02234637, 0.97765363],\n",
       "       [1.        , 0.        ],\n",
       "       [0.32417582, 0.67582418],\n",
       "       [0.09714286, 0.90285714],\n",
       "       [0.52459016, 0.47540984],\n",
       "       [0.71568627, 0.28431373],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.59444444, 0.40555556],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.21818182, 0.78181818],\n",
       "       [0.80978261, 0.19021739],\n",
       "       [0.05699482, 0.94300518],\n",
       "       [1.        , 0.        ],\n",
       "       [0.83928571, 0.16071429],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01714286, 0.98285714],\n",
       "       [0.15979381, 0.84020619],\n",
       "       [0.02873563, 0.97126437],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99470899, 0.00529101],\n",
       "       [0.9027027 , 0.0972973 ],\n",
       "       [0.11173184, 0.88826816],\n",
       "       [0.915     , 0.085     ],\n",
       "       [0.01036269, 0.98963731],\n",
       "       [0.61083744, 0.38916256],\n",
       "       [0.08333333, 0.91666667],\n",
       "       [0.98360656, 0.01639344],\n",
       "       [0.77435897, 0.22564103],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.89444444, 0.10555556],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.27692308, 0.72307692],\n",
       "       [0.99      , 0.01      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.89714286, 0.10285714],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.76216216, 0.23783784],\n",
       "       [0.91764706, 0.08235294],\n",
       "       [1.        , 0.        ],\n",
       "       [0.68396226, 0.31603774],\n",
       "       [0.47692308, 0.52307692],\n",
       "       [0.        , 1.        ],\n",
       "       [0.8956044 , 0.1043956 ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.87209302, 0.12790698],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.80625   , 0.19375   ],\n",
       "       [0.10416667, 0.89583333],\n",
       "       [0.46842105, 0.53157895],\n",
       "       [0.25      , 0.75      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.89528796, 0.10471204],\n",
       "       [0.75520833, 0.24479167],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03174603, 0.96825397],\n",
       "       [0.92178771, 0.07821229],\n",
       "       [0.96335079, 0.03664921],\n",
       "       [1.        , 0.        ],\n",
       "       [0.52662722, 0.47337278],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99492386, 0.00507614],\n",
       "       [0.00537634, 0.99462366],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97282609, 0.02717391],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05789474, 0.94210526],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99473684, 0.00526316],\n",
       "       [0.01111111, 0.98888889],\n",
       "       [1.        , 0.        ],\n",
       "       [0.13661202, 0.86338798],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.45555556, 0.54444444],\n",
       "       [0.09444444, 0.90555556],\n",
       "       [0.20786517, 0.79213483],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97175141, 0.02824859],\n",
       "       [0.18781726, 0.81218274],\n",
       "       [0.98947368, 0.01052632],\n",
       "       [0.0052356 , 0.9947644 ],\n",
       "       [0.00591716, 0.99408284],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98351648, 0.01648352],\n",
       "       [0.33160622, 0.66839378],\n",
       "       [0.99431818, 0.00568182],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99415205, 0.00584795],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04712042, 0.95287958],\n",
       "       [0.9947644 , 0.0052356 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03409091, 0.96590909],\n",
       "       [0.67914439, 0.32085561]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oob decision function for each training instance is also available through the oob_decision_function_ variable.\n",
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c1fa63",
   "metadata": {},
   "source": [
    "## Random Patches and Random Subspaces\n",
    "BaggingClassifier class supports sampling the features as well.This is controlled\n",
    "by two hyperparameters:\n",
    "- max_features \n",
    "- bootstrap_features.\n",
    "\n",
    "Thus, each predictor will be trained on a random subset of the input features.\n",
    "This is particularly useful when you are dealing with high-dimensional inputs (such\n",
    "as images). <br>\n",
    "Sampling both training instances and features is called the **Random\n",
    "Patches method**. <br>\n",
    "Keeping all training instances (i.e., bootstrap=False and max_sam\n",
    "ples=1.0) but sampling features (i.e., bootstrap_features=True and/or max_fea\n",
    "tures smaller than 1.0) is called the **Random Subspaces method**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896bb2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
