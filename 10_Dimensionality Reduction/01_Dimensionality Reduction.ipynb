{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0caea3d",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "**Curse of dimensionality:**  thousands or even millions of features make training extremely slow, it can also\n",
    "make it much harder to find a good solution. <br>\n",
    "Fortunately, in real-world problems, we can turn an intractable problem into a tractable one.\n",
    "\n",
    "Reducing dimensionality does lose some information, so even though it will speed up training, it may also make system perform\n",
    "slightly worse. It also makes pipelines a bit more complex and thus harder to maintain. <br>\n",
    "First try to train system with the original data before considering using dimensionality\n",
    "reduction if training is too slow. <br>\n",
    "In some cases, however,\n",
    "reducing the dimensionality of the training data may filter out\n",
    "some noise and unnecessary details and thus result in higher performance\n",
    "(but in general it wonâ€™t; it will just speed up training).\n",
    "\n",
    "**Highdimensional datasets** are at risk of being very sparse: most training instances are\n",
    "likely to be far away from each other. Of course, this also means that a new instance\n",
    "will likely be far away from any training instance, making predictions much less reliable\n",
    "than in lower dimensions, since they will be based on much larger extrapolations.\n",
    "In short, the more dimensions the training set has, the greater the risk of overfitting it. <br>\n",
    "\n",
    "In theory, one solution to the curse of dimensionality could be to increase the size of\n",
    "the training set to reach a sufficient density of training instances. Unfortunately, in\n",
    "practice, the number of training instances required to reach a given density grows\n",
    "exponentially with the number of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08917d4",
   "metadata": {},
   "source": [
    "## Main Approaches for Dimensionality Reduction\n",
    "### Projection\n",
    "\n",
    "In most real-world problems, training instances are not spread out uniformly across\n",
    "all dimensions. Many features are almost constant, while others are highly correlated. As a result, all training instances actually lie within (or close to) a much lower-dimensional subspace of the high-dimensional space. <br>\n",
    "If we project every training instance perpendicularly onto lower-dimensional subspace, we get the new lower-dimensional dataset. <br>\n",
    "However, projection is not always the best approach to dimensionality reduction. In\n",
    "many cases the subspace may twist and turn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0d8acf",
   "metadata": {},
   "source": [
    "### Manifold Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de08f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
