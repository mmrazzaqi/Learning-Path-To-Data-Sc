{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ba01088",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "A **Support Vector Machine (SVM)** is a very powerful and versatile Machine Learning\n",
    "model, capable of performing linear or nonlinear classification, regression, and even\n",
    "outlier detection.\n",
    "\n",
    "SVMs are particularly well suited for **classification of complex but small- or medium-sized datasets.**\n",
    "\n",
    "## Linear SVM Classification\n",
    "**On Left** the model whose decision boundary is represented by the dashed line is so bad that it\n",
    "does not even separate the classes properly. The other **two models work perfectly on\n",
    "this training set, but their decision boundaries come so close to the instances that\n",
    "these models will probably not perform as well on new instances.**\n",
    "\n",
    "In contrast, the solid line in the plot on the right represents the decision boundary of an SVM classifier;\n",
    "this line not only **separates the two classes but also stays as far away from the\n",
    "closest training instances as possible.** You can think of an SVM classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes.\n",
    "This is called **large margin classification.**\n",
    "\n",
    "<img src=\"images/svm_concept.jpg\" width='800' />\n",
    "\n",
    "Notice that adding more training instances “off the street” will not affect the decision\n",
    "boundary at all: it is fully determined (or “supported”) by the instances located on the\n",
    "edge of the street. These instances are called the **support vectors** (they are circled in above fig).\n",
    "\n",
    "#### SVMs are sensitive to the feature scales.\n",
    "- On the left plot, the vertical scale is much larger than the horizontal scale, so the widest possible street is close to horizontal.\n",
    "\n",
    "- After feature scaling the decision boundary looks much better.\n",
    "\n",
    "<img src=\"images/sensitivity_to_feature_scales.jpg\" width='800' />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a58bca",
   "metadata": {},
   "source": [
    "## Soft Margin Classification\n",
    "\n",
    "If we strictly impose that all instances be off the street and on the right side, this is\n",
    "called **hard margin classification.** \n",
    "\n",
    "Two issues with it\n",
    "- it only works if the data is linearly separable\n",
    "- it is quite sensitive to outliers\n",
    "\n",
    "<img src=\"images/hard_soft_margin.png\" width='600' />\n",
    "\n",
    "To avoid these issues it is preferable to use a more flexible model. The objective is to\n",
    "find a good balance between keeping the street as large as possible and limiting the\n",
    "margin violations (i.e., instances that end up in the middle of the street or even on the\n",
    "wrong side). This is called **soft margin classification.**\n",
    "\n",
    "##### C hyperparameter :\n",
    "- a smaller C value leads to a wider street but more margin violations.\n",
    "- high C value the classifier makes fewer margin violations but ends up with a smaller margin. However,\n",
    "it seems likely that the **first classifier will generalize better:** in fact even on this training\n",
    "set it makes fewer prediction errors, since most of the margin violations are\n",
    "actually on the correct side of the decision boundary.\n",
    "\n",
    "<img src=\"images/large_margin_vs_lower_margin.jpg\" width='800' />\n",
    "\n",
    "- If SVM model is overfitting, you can try regularizing it by reducing C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ee37e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "872ac01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('linear_scv', LinearSVC(C=1, loss='hinge'))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "X= iris['data'][:,(2,3)]   # petal length, petal width\n",
    "y=(iris['target']==2).astype(np.float64) # Iris-Virginica\n",
    "\n",
    "svm_clf = Pipeline([('scaler', StandardScaler()),\n",
    "                   ('linear_scv', LinearSVC(C=1, loss='hinge'))])\n",
    "\n",
    "svm_clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c6c52dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b2b5dd",
   "metadata": {},
   "source": [
    "- Unlike Logistic Regression classifiers, SVM classifiers do not output\n",
    "probabilities for each class.\n",
    "- Alternatively, you could use the SVC class, using SVC(kernel=\"linear\", C=1), but it\n",
    "is much slower, especially with large training sets\n",
    "- Another\n",
    "option is to use the SGDClassifier class, with SGDClassifier **(loss=\"hinge\",\n",
    "alpha=1/(m*C)).** This applies regular Stochastic Gradient Descent to\n",
    "train a linear SVM classifier. It does not converge as fast as the LinearSVC class, but it\n",
    "can be useful to handle huge datasets that do not fit in memory (out-of-core training),\n",
    "or to handle online classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f449bf18",
   "metadata": {},
   "source": [
    "## Nonlinear SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dcdfdc",
   "metadata": {},
   "source": [
    "Adding features to make a dataset linearly separable\n",
    "\n",
    "<img src=\"images/adding_features_make_dataset_lin_separable.jpg\" width='600' />\n",
    "\n",
    "Left not linear separable, But if we add a second feature\n",
    "x2 = (x1)2, the resulting 2D dataset on right is perfectly linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b077f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('poly_features', PolynomialFeatures(degree=3)),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('svm_clf', LinearSVC(C=10, loss='hinge'))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "polynomial_svm_clf = Pipeline([(\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "                               (\"scaler\", StandardScaler()),\n",
    "                               (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))])\n",
    "polynomial_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43363002",
   "metadata": {},
   "source": [
    "## Polynomial Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161d70aa",
   "metadata": {},
   "source": [
    "- Adding polynomial features is simple to implement and can work great with all sorts of ML algorithms. but at a low polynomial degree it cannot deal with very complex datasets, and with a high polynomial degree it creates a huge number of features, making the model too slow.\n",
    "- When using SVMs we can apply an almost miraculous mathematical technique called the **kernel trick**. It makes it possible to\n",
    "  get the same result as if we added many polynomial features, even with very highdegree polynomials, without actually having to   add them.\n",
    "  \n",
    "This trick is implemented by the SVC class (**kernel trick**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61bcbcdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=5, coef0=1, kernel='poly'))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "poly_kernel_svmclf = Pipeline([('scaler', StandardScaler()),\n",
    "                              ('svm_clf', SVC(C=5, kernel='poly', degree=3, coef0=1))])\n",
    "\n",
    "poly_kernel_svmclf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81169a",
   "metadata": {},
   "source": [
    "If our model is overfitting, we might want to reduce the polynomial degree. Conversely, if it is underfitting, we can try increasing it. <br>\n",
    "The **hyperparameter coef0** controls how much the model is influenced by highdegree\n",
    "polynomials versus low-degree polynomials.\n",
    "\n",
    "<img src=\"images/svm_clf_poly_kernel.jpg\" width='600' />\n",
    "\n",
    "A common approach to find the right hyperparameter values is to\n",
    "use grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57603ab4",
   "metadata": {},
   "source": [
    "## Adding Similarity Features\n",
    "\n",
    "Another technique to tackle nonlinear problems is to add features computed using a\n",
    "similarity function that measures how much each instance resembles a particular\n",
    "landmark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec06767",
   "metadata": {},
   "source": [
    "### Gaussian Radial Basis Function (RBF) Kernel\n",
    "\n",
    "Just like the polynomial features method, the similarity features method can be useful\n",
    "with any Machine Learning algorithm, but it may be computationally expensive to\n",
    "compute all the additional features, especially on large training sets. However, once\n",
    "again the kernel trick does its SVM magic: it makes it possible to obtain a similar\n",
    "result as if you had added many similarity features, without actually having to add\n",
    "them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d50ca9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=0.001, gamma=5))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([(\"scaler\", StandardScaler()),\n",
    "                               (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))])\n",
    "\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b72e38",
   "metadata": {},
   "source": [
    "As a rule of thumb, you should always try the linear\n",
    "kernel first (remember that LinearSVC is much faster than SVC(ker\n",
    "nel=\"linear\")), especially if the training set is very large or if it\n",
    "has plenty of features. If the training set is not too large, you should\n",
    "try the Gaussian RBF kernel as well; it works well in most cases.\n",
    "\n",
    "### Computational Complexity\n",
    "\n",
    "<img src=\"images/complexity_table.png\" width='800' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017dca9d",
   "metadata": {},
   "source": [
    "## SVM Regression\n",
    "\n",
    "SVM algorithm also supports linear and nonlinear\n",
    "regression. The trick is to reverse the objective: instead of trying to fit the largest possible\n",
    "street between two classes while limiting margin violations, SVM Regression\n",
    "tries to fit as many instances as possible on the street while limiting margin violations\n",
    "(i.e., instances off the street). The width of the street is controlled by a hyperparameter\n",
    "ϵ.\n",
    "\n",
    "<img src=\"images/svm_reg.jpg\" width='600' />\n",
    "\n",
    "Adding more training instances within the margin does not affect the model’s predictions;\n",
    "thus, the model is said to be ϵ-insensitive.\n",
    "\n",
    "Scikit-Learn’s LinearSVR class to perform linear SVM Regression is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "804c9965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(epsilon=1.5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "svm_reg = LinearSVR(epsilon=1.5)\n",
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ab5602",
   "metadata": {},
   "source": [
    "To tackle nonlinear regression tasks, we can use a kernelized SVM model. Scikit-Learn’s SVR class (which supports the kernel trick) is equivalent of the SVC class, and the LinearSVR class is the regression equivalent\n",
    "of the LinearSVC class. The LinearSVR class scales linearly with the size of the training\n",
    "set (just like the LinearSVC class), while the SVR class gets much too slow when\n",
    "the training set grows large (just like the SVC class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e2bdf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100, degree=2, kernel='poly')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svm_poly_reg = SVR(kernel='poly', degree=2, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a0d33",
   "metadata": {},
   "source": [
    "## Under the Hood\n",
    "### Decision Function and Predictions\n",
    "\n",
    "The linear SVM classifier model predicts the class of a new instance **x** by simply computing\n",
    "the decision function $ w^T.x + b = w_1.x_1 + ⋯ + w_n.x_n + b: $ if the result is positive,\n",
    "the predicted class **ŷ** is the positive class (1), or else it is the negative class (0);\n",
    "\n",
    "<img src=\"images/lin_svm_pred.png\" width='400' />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f98a777",
   "metadata": {},
   "source": [
    "## Training Objective\n",
    "\n",
    "Consider the slope of the decision function: it is equal to the norm of the weight vector,\n",
    "$ ∥ w ∥ $. If we divide this slope by 2, the points where the decision function is equal\n",
    "to $ ±1 $ are going to be twice as far away from the decision boundary. In other words,\n",
    "dividing the slope by 2 will multiply the margin by 2.\n",
    "\n",
    "<img src=\"images/w_vs_margin.jpg\" width='400' />\n",
    "\n",
    "The smaller the weight vector w, the larger the margin.\n",
    "\n",
    "So we want to minimize $ ∥ w ∥ $ to get a large margin. However, if we also want to avoid\n",
    "any margin violation (hard margin), then we need the decision function to be greater\n",
    "than 1 for all positive training instances, and lower than –1 for negative training\n",
    "instances. If we define $ t^i = –1 $ for negative instances (if $ y^i = 0) $ and $t^i = 1$ for positive\n",
    "instances (if $y^i = 1)$, then we can express this constraint as $t^i(w^T x^i + b) ≥ 1$ for all\n",
    "instances.\n",
    "\n",
    "<img src=\"images/svm_eq.png\" width='400' />\n",
    "\n",
    "We can therefore express the hard margin linear SVM classifier objective as the constrained\n",
    "optimization problem\n",
    "\n",
    "\n",
    "<img src=\"images/svm_eq_2.jpg\" width='600' />\n",
    "\n",
    "We are minimizing $1/2w^T w$, which is equal to $1/2\n",
    "∥ w ∥^2$, rather than\n",
    "minimizing $∥ w ∥$. Indeed, $1/2\n",
    "∥ w ∥^2$ has a nice and simple derivative\n",
    "(it is just w) while $∥ w ∥$ is not differentiable at w = 0. Optimization\n",
    "algorithms work much better on differentiable functions.\n",
    "\n",
    "To get the **soft margin** objective, we need to introduce a **slack variable** $ ζ^i ≥ 0 $ for each\n",
    "instance. $ ζ^i $ measures how much the ith instance is allowed to violate the margin. We\n",
    "now have two conflicting objectives: making the slack variables as small as possible to\n",
    "reduce the margin violations, and making $ 1/2\n",
    "w^T w $ as small as possible to increase the\n",
    "margin. This is where the **C hyperparameter** comes in: it allows us to define the trade‐off between these two objectives. This gives us the constrained optimization problem.\n",
    "\n",
    "<img src=\"images/svm_eq_3.png\" width='600' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca9c7bb",
   "metadata": {},
   "source": [
    "## Kernelized SVM\n",
    "\n",
    "Transformed training set. <br>\n",
    "\n",
    "<img src=\"images/kernelized.png\" width='600' />\n",
    "\n",
    "Notice that the transformed vector is three-dimensional instead of two-dimensional. <br>\n",
    "Now let’s look at what happens to a couple of two-dimensional vectors, **a** and **b**, if we\n",
    "apply this 2nd-degree polynomial mapping and then compute the dot product of the\n",
    "transformed vectors.\n",
    "\n",
    "<img src=\"images/kernelized_2.png\" width='600' />\n",
    "\n",
    "The dot product of the transformed vectors is equal to the square of\n",
    "the dot product of the original vectors: $ ϕ(a)^T ϕ(b) = (a^T b)^2 $.\n",
    "\n",
    "Don’t actually need to transform the training instances at all: just replace the dot\n",
    "product by its square. The result will be strictly the same as if you\n",
    "went through the trouble of actually transforming the training set then fitting a linear\n",
    "SVM algorithm, but this trick makes the whole process much more computationally\n",
    "efficient. This is the essence of the **kernel trick.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61dba75",
   "metadata": {},
   "source": [
    "## Online SVMs\n",
    "\n",
    "Online learning means learning incrementally, typically as new instances arrive.\n",
    "\n",
    "<img src=\"images/loss.png\" width='600' />\n",
    "\n",
    "The first sum in the cost function will push the model to have a small weight vector\n",
    "$w$, leading to a larger margin. The second sum computes the total of all margin violations.\n",
    "An instance’s margin violation is equal to 0 if it is located off the street and on\n",
    "the correct side, or else it is proportional to the distance to the correct side of the\n",
    "street. Minimizing this term ensures that the model makes the margin violations as\n",
    "small and as few as possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eeb3eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
