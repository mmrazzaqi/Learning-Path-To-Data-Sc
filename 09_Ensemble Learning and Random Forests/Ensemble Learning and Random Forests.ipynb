{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b75047",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "If we aggregate\n",
    "the predictions of a group of predictors (such as classifiers or regressors), you will\n",
    "often get better predictions than with the best individual predictor. A group of predictors\n",
    "is called an **ensemble**; thus, this technique is called **Ensemble Learning**, and an\n",
    "Ensemble Learning algorithm is called an **Ensemble method**.\n",
    "\n",
    "Such an ensemble (Decision Tree classifiers) of Decision Trees is called a **Random Forest**\n",
    "\n",
    "Popular Ensemble methods, including **bagging,\n",
    "boosting, stacking**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff3100",
   "metadata": {},
   "source": [
    "## Voting Classifiers\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/hard_voting_clf.jpg\" width='600' />\n",
    "\n",
    "Voting classifier often achieves a higher accuracy than the\n",
    "best classifier in the ensemble.\n",
    "\n",
    "Suppose you build an ensemble containing 1,000 classifiers that are individually\n",
    "correct only 51% of the time (barely better than random guessing). If you predict\n",
    "the majority voted class, you can hope for up to 75% accuracy! However, this is\n",
    "only true if all classifiers are perfectly independent, making uncorrelated errors,\n",
    "which is clearly not the case since they are trained on the same data. They are likely to\n",
    "make the same types of errors, so there will be many majority votes for the wrong\n",
    "class, reducing the ensemble’s accuracy.\n",
    "- Ensemble methods work best when the predictors are as independent\n",
    "from one another as possible. One way to get diverse classifiers\n",
    "is to train them using very different algorithms. This increases the\n",
    "chance that they will make very different types of errors, improving\n",
    "the ensemble’s accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af112617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eef8e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f50253de",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d79be76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svm', SVC())])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(estimators= [('lr', log_clf), ('rf', rnd_clf), ('svm', svm_clf)], \n",
    "                              voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4f02e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.88\n",
      "SVC 0.896\n",
      "VotingClassifier 0.904\n"
     ]
    }
   ],
   "source": [
    "# Let’s look at each classifier’s accuracy on the test set\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca41a731",
   "metadata": {},
   "source": [
    "If all classifiers are able to estimate class probabilities (i.e., they have a pre\n",
    "dict_proba() method), then we can predict the class with the\n",
    "highest class probability, averaged over all the individual classifiers. This is called **soft\n",
    "voting.** <br>\n",
    "It often achieves higher performance than hard voting because it gives more\n",
    "weight to highly confident votes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c795784",
   "metadata": {},
   "source": [
    "## Bagging and Pasting\n",
    "\n",
    "One way to get a diverse set of classifiers is to use very different training algorithms. <br>\n",
    "**Bagging:** Another approach is to use the same training algorithm for every predictor, but to train them on **different random subsets of the training set.**\n",
    "- When sampling is performed with replacement, this method is called bagging (short for\n",
    "bootstrap aggregating). <br>\n",
    "- When sampling is performed without replacement, it is called **pasting.**\n",
    "\n",
    "**Sample with replacement:** The two sample values are independent. Practically, this means that what we get on the first one doesn't affect what we get on the second. Mathematically, this means that the covariance between the two is zero. <br>\n",
    "The outcome of the first draw does not affect the probability of the outcome on the second draw.\n",
    "\n",
    "<br>\n",
    "\n",
    "When we **sample without replacement**, the items in the sample are dependent because the outcome of one random draw is affected by the previous draw.\n",
    "\n",
    "<img src=\"images/bagging_1.png\" width='700' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d282ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), \n",
    "                            n_estimators=500, \n",
    "                            max_samples=100,\n",
    "                            bootstrap=True,\n",
    "                            n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26d0a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fae1f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a61f9c3",
   "metadata": {},
   "source": [
    "The BaggingClassifier automatically performs soft voting\n",
    "instead of hard voting if the base classifier can estimate class probabilities\n",
    "(i.e., if it has a predict_proba() method), which is the case\n",
    "with Decision Trees classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81b3296",
   "metadata": {},
   "source": [
    "## Out-of-Bag Evaluation\n",
    "\n",
    "With bagging, some instances may be sampled several times for any given predictor,\n",
    "while others may not be sampled at all. <br>\n",
    "The remaining training instances that are not sampled are called out-of-bag (oob) instances. <br>\n",
    "Since a predictor never sees the oob instances during training, it can be evaluated on\n",
    "these instances, without the need for a separate validation set. We can evaluate the\n",
    "ensemble itself by averaging out the oob evaluations of each predictor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d3221cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=500,\n",
       "                  n_jobs=-1, oob_score=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# automatic oob evaluation after training\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                            n_estimators=500,\n",
    "                            bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4d67bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8986666666666666"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9f592ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let’s verify this accuracy of oob eveluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bd3f954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36945813, 0.63054187],\n",
       "       [0.31638418, 0.68361582],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.10465116, 0.89534884],\n",
       "       [0.38372093, 0.61627907],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.965     , 0.035     ],\n",
       "       [0.7679558 , 0.2320442 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.71098266, 0.28901734],\n",
       "       [0.82681564, 0.17318436],\n",
       "       [0.99456522, 0.00543478],\n",
       "       [0.06451613, 0.93548387],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98882682, 0.01117318],\n",
       "       [0.94565217, 0.05434783],\n",
       "       [0.98857143, 0.01142857],\n",
       "       [0.01098901, 0.98901099],\n",
       "       [0.32085561, 0.67914439],\n",
       "       [0.92105263, 0.07894737],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98492462, 0.01507538],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.65555556, 0.34444444],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.16091954, 0.83908046],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.33684211, 0.66315789],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.22282609, 0.77717391],\n",
       "       [0.38505747, 0.61494253],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02824859, 0.97175141],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00480769, 0.99519231],\n",
       "       [0.99450549, 0.00549451],\n",
       "       [0.94270833, 0.05729167],\n",
       "       [0.97777778, 0.02222222],\n",
       "       [0.97883598, 0.02116402],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03529412, 0.96470588],\n",
       "       [0.99      , 0.01      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01041667, 0.98958333],\n",
       "       [0.9893617 , 0.0106383 ],\n",
       "       [0.81407035, 0.18592965],\n",
       "       [0.36216216, 0.63783784],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.65425532, 0.34574468],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.87431694, 0.12568306],\n",
       "       [1.        , 0.        ],\n",
       "       [0.65384615, 0.34615385],\n",
       "       [0.12765957, 0.87234043],\n",
       "       [0.67005076, 0.32994924],\n",
       "       [0.91145833, 0.08854167],\n",
       "       [0.        , 1.        ],\n",
       "       [0.18857143, 0.81142857],\n",
       "       [0.87777778, 0.12222222],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02777778, 0.97222222],\n",
       "       [0.04145078, 0.95854922],\n",
       "       [0.359375  , 0.640625  ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00546448, 0.99453552],\n",
       "       [0.83597884, 0.16402116],\n",
       "       [0.00581395, 0.99418605],\n",
       "       [0.00505051, 0.99494949],\n",
       "       [0.        , 1.        ],\n",
       "       [0.27604167, 0.72395833],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.91803279, 0.08196721],\n",
       "       [0.75897436, 0.24102564],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.21301775, 0.78698225],\n",
       "       [0.72254335, 0.27745665],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02061856, 0.97938144],\n",
       "       [0.47849462, 0.52150538],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0255102 , 0.9744898 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.23      , 0.77      ],\n",
       "       [0.4673913 , 0.5326087 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02173913, 0.97826087],\n",
       "       [0.97979798, 0.02020202],\n",
       "       [0.25128205, 0.74871795],\n",
       "       [0.88601036, 0.11398964],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.83908046, 0.16091954],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00492611, 0.99507389],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98870056, 0.01129944],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00628931, 0.99371069],\n",
       "       [0.95348837, 0.04651163],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00543478, 0.99456522],\n",
       "       [0.26804124, 0.73195876],\n",
       "       [0.96256684, 0.03743316],\n",
       "       [0.30808081, 0.69191919],\n",
       "       [0.98453608, 0.01546392],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.66477273, 0.33522727],\n",
       "       [0.41081081, 0.58918919],\n",
       "       [0.40340909, 0.59659091],\n",
       "       [0.87179487, 0.12820513],\n",
       "       [0.91061453, 0.08938547],\n",
       "       [0.05555556, 0.94444444],\n",
       "       [0.78142077, 0.21857923],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0052356 , 0.9947644 ],\n",
       "       [0.98734177, 0.01265823],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00595238, 0.99404762],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01538462, 0.98461538],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94736842, 0.05263158],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99447514, 0.00552486],\n",
       "       [0.        , 1.        ],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.24022346, 0.75977654],\n",
       "       [0.00552486, 0.99447514],\n",
       "       [0.        , 1.        ],\n",
       "       [0.31016043, 0.68983957],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00537634, 0.99462366],\n",
       "       [0.59473684, 0.40526316],\n",
       "       [0.92265193, 0.07734807],\n",
       "       [0.00531915, 0.99468085],\n",
       "       [0.98802395, 0.01197605],\n",
       "       [0.99441341, 0.00558659],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.05952381, 0.94047619],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02162162, 0.97837838],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.06321839, 0.93678161],\n",
       "       [0.99502488, 0.00497512],\n",
       "       [0.90643275, 0.09356725],\n",
       "       [0.83435583, 0.16564417],\n",
       "       [0.57142857, 0.42857143],\n",
       "       [0.        , 1.        ],\n",
       "       [0.16756757, 0.83243243],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95977011, 0.04022989],\n",
       "       [0.95631068, 0.04368932],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01098901, 0.98901099],\n",
       "       [0.        , 1.        ],\n",
       "       [0.42458101, 0.57541899],\n",
       "       [0.90217391, 0.09782609],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01183432, 0.98816568],\n",
       "       [0.00546448, 0.99453552],\n",
       "       [0.93434343, 0.06565657],\n",
       "       [0.        , 1.        ],\n",
       "       [0.26589595, 0.73410405],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9673913 , 0.0326087 ],\n",
       "       [0.81865285, 0.18134715],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.08018868, 0.91981132],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02890173, 0.97109827],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0591716 , 0.9408284 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.75824176, 0.24175824],\n",
       "       [0.00546448, 0.99453552],\n",
       "       [0.9039548 , 0.0960452 ],\n",
       "       [0.99425287, 0.00574713],\n",
       "       [0.19170984, 0.80829016],\n",
       "       [0.17751479, 0.82248521],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.2238806 , 0.7761194 ],\n",
       "       [0.96174863, 0.03825137],\n",
       "       [0.00571429, 0.99428571],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99487179, 0.00512821],\n",
       "       [0.        , 1.        ],\n",
       "       [0.51485149, 0.48514851],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.09375   , 0.90625   ],\n",
       "       [0.14583333, 0.85416667],\n",
       "       [0.98529412, 0.01470588],\n",
       "       [0.01630435, 0.98369565],\n",
       "       [1.        , 0.        ],\n",
       "       [0.42777778, 0.57222222],\n",
       "       [0.10326087, 0.89673913],\n",
       "       [0.53846154, 0.46153846],\n",
       "       [0.56185567, 0.43814433],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.61052632, 0.38947368],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.24418605, 0.75581395],\n",
       "       [0.83243243, 0.16756757],\n",
       "       [0.06557377, 0.93442623],\n",
       "       [1.        , 0.        ],\n",
       "       [0.83333333, 0.16666667],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00581395, 0.99418605],\n",
       "       [0.13065327, 0.86934673],\n",
       "       [0.03296703, 0.96703297],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.88461538, 0.11538462],\n",
       "       [0.14054054, 0.85945946],\n",
       "       [0.95588235, 0.04411765],\n",
       "       [0.01162791, 0.98837209],\n",
       "       [0.635     , 0.365     ],\n",
       "       [0.07692308, 0.92307692],\n",
       "       [0.99056604, 0.00943396],\n",
       "       [0.76704545, 0.23295455],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95789474, 0.04210526],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.31382979, 0.68617021],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.88481675, 0.11518325],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.80434783, 0.19565217],\n",
       "       [0.94117647, 0.05882353],\n",
       "       [1.        , 0.        ],\n",
       "       [0.68617021, 0.31382979],\n",
       "       [0.59067358, 0.40932642],\n",
       "       [0.        , 1.        ],\n",
       "       [0.8989899 , 0.1010101 ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.81521739, 0.18478261],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.73291925, 0.26708075],\n",
       "       [0.10294118, 0.89705882],\n",
       "       [0.50857143, 0.49142857],\n",
       "       [0.23783784, 0.76216216],\n",
       "       [0.        , 1.        ],\n",
       "       [0.88690476, 0.11309524],\n",
       "       [0.79558011, 0.20441989],\n",
       "       [0.00552486, 0.99447514],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02777778, 0.97222222],\n",
       "       [0.96703297, 0.03296703],\n",
       "       [0.95321637, 0.04678363],\n",
       "       [1.        , 0.        ],\n",
       "       [0.54651163, 0.45348837],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00543478, 0.99456522],\n",
       "       [0.98148148, 0.01851852],\n",
       "       [0.02366864, 0.97633136],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97175141, 0.02824859],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05882353, 0.94117647],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01123596, 0.98876404],\n",
       "       [1.        , 0.        ],\n",
       "       [0.11494253, 0.88505747],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.41052632, 0.58947368],\n",
       "       [0.10614525, 0.89385475],\n",
       "       [0.25698324, 0.74301676],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98901099, 0.01098901],\n",
       "       [0.20118343, 0.79881657],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00552486, 0.99447514],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96111111, 0.03888889],\n",
       "       [0.30054645, 0.69945355],\n",
       "       [0.99453552, 0.00546448],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99465241, 0.00534759],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03571429, 0.96428571],\n",
       "       [0.98882682, 0.01117318],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02139037, 0.97860963],\n",
       "       [0.65853659, 0.34146341]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oob decision function for each training instance is also available through the oob_decision_function_ variable.\n",
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c1fa63",
   "metadata": {},
   "source": [
    "## Random Patches and Random Subspaces\n",
    "BaggingClassifier class supports sampling the features as well.This is controlled\n",
    "by two hyperparameters:\n",
    "- max_features \n",
    "- bootstrap_features.\n",
    "\n",
    "Thus, each predictor will be trained on a random subset of the input features.\n",
    "This is particularly useful when you are dealing with high-dimensional inputs (such\n",
    "as images). <br>\n",
    "Sampling both training instances and features is called the **Random\n",
    "Patches method**. <br>\n",
    "Keeping all training instances (i.e., bootstrap=False and max_samples=1.0) but sampling features (i.e., bootstrap_features=True and/or max_fea\n",
    "tures smaller than 1.0) is called the **Random Subspaces method**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca52d4b",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "Ensemble of Decision Trees, generally\n",
    "trained via the bagging method (or sometimes pasting).\n",
    "Instead of building a **BaggingClassifier** and passing\n",
    "it a **DecisionTreeClassifier**, you can instead use the **RandomForestClassifier**\n",
    "class, which is more convenient and optimized for Decision Trees (The BaggingClassifier class remains useful if you want a bag of something other than Decision Trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80c78f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6706263e",
   "metadata": {},
   "source": [
    "**RandomForestClassifier** has all the hyperparameters of a\n",
    "**DecisionTreeClassifier** (to control how trees are grown), plus all the hyperparameters\n",
    "of a **BaggingClassifier** to control the ensemble itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecf81ba",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "Random Forests make it easy to measure the\n",
    "relative importance of each feature. Scikit-Learn measures a feature’s importance by\n",
    "looking at how much the tree nodes that use that feature reduce impurity on average\n",
    "(across all trees in the forest). More precisely, it is a weighted average, where each\n",
    "node’s weight is equal to the number of training samples that are associated with it.\n",
    "\n",
    "Scikit-Learn computes this score automatically for each feature after training, then it\n",
    "scales the results so that the sum of all importances is equal to 1. You can access the\n",
    "result using the **feature_importances_** variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60f64b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09120139699453958\n",
      "sepal width (cm) 0.023989731426464052\n",
      "petal length (cm) 0.4226600768284621\n",
      "petal width (cm) 0.4621487947505343\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f23fb0",
   "metadata": {},
   "source": [
    "Random Forests are very handy to get a quick understanding of what features\n",
    "actually matter, in particular if you need to perform feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d5ff9",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "Boosting (originally called hypothesis boosting) refers to any Ensemble method that\n",
    "can combine several weak learners into a strong learner. The general idea of most\n",
    "boosting methods is to **train predictors sequentially, each trying to correct its predecessor.**\n",
    "\n",
    "<img src=\"images/boosting.png\" width='700' />\n",
    "\n",
    "<br>\n",
    "\n",
    "Most popular boosting methods are:\n",
    "- AdaBoost(Adaptive Boosting)\n",
    "- Gradient Boosting\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fa91c5",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "One way for a new predictor to correct its predecessor is **to pay a bit more attention\n",
    "to the training instances that the predecessor underfitted.** This results in new predictors\n",
    "focusing more and more on the hard cases. This is the technique used by Ada‐\n",
    "Boost.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/adaboost.jpg\" width='700' />\n",
    "\n",
    "<br>\n",
    "\n",
    "The first classifier gets many instances wrong, so their weights get boosted. The second classifier therefore does a better job on these instances, and\n",
    "so on.\n",
    "\n",
    "There is one **important drawback** to this sequential learning technique:\n",
    "**it cannot be parallelized (or only partially),** since each predictor\n",
    "can only be trained after the previous predictor has been\n",
    "trained and evaluated. As a result, it does not scale as well as bagging\n",
    "or pasting.\n",
    "\n",
    "\n",
    "\n",
    "Scikit-Learn actually uses a multiclass version of AdaBoost called **SAMME** (which\n",
    "stands for Stagewise Additive Modeling using a Multiclass Exponential loss function).\n",
    "When there are just two classes, SAMME is equivalent to AdaBoost.\n",
    "\n",
    "The following code trains an AdaBoost classifier based on 200 **Decision Stumps** using\n",
    "Scikit-Learn’s AdaBoostClassifier class. A **Decision Stump is a Decision Tree with max_depth=1—in\n",
    "other words, a tree composed of a single decision node plus two leaf nodes.** This is\n",
    "the default base estimator for the AdaBoostClassifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "077c8e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "DecisionTreeClassifier(max_depth=1), \n",
    "    n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", \n",
    "    learning_rate=0.5)\n",
    "\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60e2b2d",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "Gradient Boosting works by sequentially adding predictors to an ensemble, each one\n",
    "correcting its predecessor. However, instead of tweaking the instance weights at every\n",
    "iteration like AdaBoost does, this method tries to fit the new predictor to the residual\n",
    "errors made by the previous predictor.\n",
    "\n",
    "Gradient Boosting also works great with regression tasks. This is\n",
    "called Gradient **Tree Boosting**, or **Gradient Boosted Regression Trees (GBRT).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c5fa443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let’s go through a simple regression example using Decision Trees as the base predictors.\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb54bd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now train a second DecisionTreeRegressor on the residual errors made by the first predictor\n",
    "\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a99eac31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then we train a third regressor on the residual errors made by the second predictor\n",
    "\n",
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8449d2cb",
   "metadata": {},
   "source": [
    "Now we have an ensemble containing three trees. It can make predictions on a new\n",
    "instance simply by adding up the predictions of all the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "866244d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sum(tree.predict(X_test) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508b685",
   "metadata": {},
   "source": [
    "<img src=\"images/gradient_boosting.jpg\" width='700' />\n",
    "\n",
    "**Figure** represents the predictions of these three trees in the left column, and the\n",
    "ensemble’s predictions in the right column. In the first row, the ensemble has just one\n",
    "tree, so its predictions are exactly the same as the first tree’s predictions. In the second\n",
    "row, a new tree is trained on the residual errors of the first tree. On the right you can\n",
    "see that the ensemble’s predictions are equal to the sum of the predictions of the first\n",
    "two trees. Similarly, in the third row another tree is trained on the residual errors of\n",
    "the second tree. You can see that the ensemble’s predictions gradually get better as\n",
    "trees are added to the ensemble.\n",
    "\n",
    "Scikit-Learn’s **GradientBoostingRegressor** class. Much like the RandomForestRegressor class, it has hyperparameters to\n",
    "control the growth of Decision Trees (e.g., max_depth, min_samples_leaf, and so on),\n",
    "as well as hyperparameters to control the ensemble training, such as the number of\n",
    "trees (n_estimators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19e25d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52544eb4",
   "metadata": {},
   "source": [
    "The learning_rate hyperparameter scales the contribution of each tree. If you set it\n",
    "to a low value, such as 0.1, you will need more trees in the ensemble to fit the training\n",
    "set, but the predictions will usually generalize better. This is a regularization technique\n",
    "called **shrinkage.**\n",
    "\n",
    "<img src=\"images/gradient_boosting_2.jpg\" width='700' />\n",
    "\n",
    "\n",
    "**Figure** shows two GBRT ensembles trained with a low\n",
    "learning rate: the one on the left does not have enough trees to fit the training set,\n",
    "while the one on the right has too many trees and overfits the training set.\n",
    "\n",
    "In order to find the optimal number of trees, you can use **early stopping**. A simple way to implement this is to use the staged_predict() method: it\n",
    "returns an iterator over the predictions made by the ensemble at each stage of training\n",
    "(with one tree, two trees, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66cf0636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=88)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred) \n",
    "          for y_pred in gbrt.staged_predict(X_val)]\n",
    "\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
    "\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e1ec1a",
   "metadata": {},
   "source": [
    "It is also possible to implement early stopping by actually stopping training early\n",
    "(instead of training a large number of trees first and then looking back to find the\n",
    "optimal number). You can do so by setting warm_start=True, which makes Scikit-\n",
    "Learn keep existing trees when the fit() method is called, allowing incremental\n",
    "training.\n",
    "\n",
    "The following code stops training when the validation error does not\n",
    "improve for five iterations in a row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "221fbb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break # early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef458ea",
   "metadata": {},
   "source": [
    "The **GradientBoostingRegressor** class also supports a **subsample** hyperparameter,\n",
    "which specifies the fraction of training instances to be used for training each tree. For\n",
    "example, if subsample=0.25, then each tree is trained on 25% of the training instances,\n",
    "selected randomly. As you can probably guess by now, this trades a higher bias\n",
    "for a lower variance. It also speeds up training considerably. **This technique is called\n",
    "Stochastic Gradient Boosting.**\n",
    "\n",
    "\n",
    "It is worth noting that an optimized implementation of Gradient Boosting is available\n",
    "in the popular python library **XGBoost**, which stands for **Extreme Gradient Boosting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89c569f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ebabdd",
   "metadata": {},
   "source": [
    "XGBoost also offers several nice features, such as **automatically taking care of early\n",
    "stopping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94dccd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.41028\n",
      "[1]\tvalidation_0-rmse:0.36708\n",
      "[2]\tvalidation_0-rmse:0.34373\n",
      "[3]\tvalidation_0-rmse:0.33505\n",
      "[4]\tvalidation_0-rmse:0.33242\n",
      "[5]\tvalidation_0-rmse:0.33519\n",
      "[6]\tvalidation_0-rmse:0.33227\n",
      "[7]\tvalidation_0-rmse:0.33541\n",
      "[8]\tvalidation_0-rmse:0.33165\n",
      "[9]\tvalidation_0-rmse:0.33338\n",
      "[10]\tvalidation_0-rmse:0.33470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python\\python3_9_7\\lib\\site-packages\\xgboost\\sklearn.py:793: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "xgb_reg.fit(X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
    "\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc4aebe",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "\n",
    "**stacking** (short for\n",
    "stacked generalization) it is based on a simple idea: instead of using trivial functions\n",
    "(such as hard voting) to aggregate the predictions of all predictors in an ensemble,\n",
    "why don’t we train a model to perform this aggregation.\n",
    "\n",
    "Figure shows such an ensemble performing a regression task on a new instance. Each of the three\n",
    "predictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor\n",
    "(called a blender, or a meta learner) takes these predictions as inputs and makes the\n",
    "final prediction (3.0).\n",
    "\n",
    "<img src=\"images/stacking_2.jpg\" width='500' />\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/stacking.jpeg\" width='700' />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6369f5dc",
   "metadata": {},
   "source": [
    "To train the blender, a common approach is to use a hold-out set.\n",
    "First, the training set is split in two subsets. The first subset is used to train the\n",
    "predictors in the first layer.\n",
    "\n",
    "Next, the first layer predictors are used to make predictions on the second (held-out)\n",
    "set. This ensures that the predictions are “clean,” since the predictors\n",
    "never saw these instances during training. Now for each instance in the hold-out set there are three predicted values. We can create a new training set using these predicted\n",
    "values as input features (which makes this new training set three-dimensional),\n",
    "and keeping the target values. The blender is trained on this new training set, so it\n",
    "learns to predict the target value given the first layer’s predictions.\n",
    "\n",
    "<img src=\"images/stacking_3.jpg\" width='400' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62934e4d",
   "metadata": {},
   "source": [
    "It is actually possible to train several different blenders this way (e.g., one using Linear\n",
    "Regression, another using Random Forest Regression, and so on): we get a whole\n",
    "layer of blenders. The trick is to split the training set into three subsets: the first one is\n",
    "used to train the first layer, the second one is used to create the training set used to\n",
    "train the second layer (using predictions made by the predictors of the first layer),\n",
    "and the third one is used to create the training set to train the third layer (using predictions\n",
    "made by the predictors of the second layer). Once this is done, we can make\n",
    "a prediction for a new instance by going through each layer sequentially.\n",
    "\n",
    "<img src=\"images/stacking_4.jpg\" width='400' />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8879200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
